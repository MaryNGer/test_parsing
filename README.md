# Анализ задачи: Сбор данных с сайта "Quotes to Scrape"

## Что было сделано

В рамках задания были реализованы два скрипта для сбора данных с сайта: синхронный - `main.py` и асинхронный - `main_async.py`. 

Данные, собранные для каждой цитаты:

- Текст
- Автор
- Ссылка на автора
- Список тегов

После сбора данных они сохраняются в JSON-файл.

## Откуда были получены данные

Информация была получена с сайта [Quotes to Scrape](https://quotes.toscrape.com/), где представлена база данных высказываний знаменитых писателей. Сайт удобно организован, что облегчает процедуру парсинга.

## Как осуществлялся сбор

### Синхронный парсинг

Сбор данных для **синхронного** парсинга осуществлялся с использованием следующих шагов:

1. **Получение страницы**: Запрашивается страница сайта с помощью библиотеки `requests`.
2. **Парсинг HTML**: Используя библиотеку `BeautifulSoup`, скрипт парсит HTML-код страницы для извлечения данных о цитатах.
3. **Извлечение данных**: Для каждой цитаты извлекается:
   - Текст цитаты
   - Имя автора
   - Ссылка на автора
   - Список тегов
4. **Сохранение данных**: Извлеченные данные добавляются в глобальный словарь `quote_dct`.
5. **Переход на следующие страницы**: Если на странице есть кнопка для перехода на следующую страницу, скрипт повторяет шаги 1-4 для каждой последующей страницы.
6. **Сохранение в JSON**: После завершения сбора данных, они сохраняются в JSON-файл с помощью библиотеки `json`.

### Асинхронный парсинг

Сбор данных для **асинхронного** парсинга осуществлялся с использованием следующих шагов:

1. **Создание асинхронной сессии**: Создание сессии `aiohttp.ClientSession` для выполнения асинхронных HTTP-запросов.
2. **Получение страниц**: Асинхронно запрашиваются страницы с цитатами. В данном примере запрашиваются первые 10 страниц. Для сбора изменяемого количества страниц можно использовать дополнительную функцию для корректного определения количества страниц.
3. **Парсинг HTML**: Используя библиотеку `BeautifulSoup`, скрипт парсит HTML-код страницы для извлечения данных о цитатах.
4. **Извлечение данных**: Для каждой цитаты извлекаются:
   - Текст цитаты
   - Имя автора
   - Ссылка на автора
   - Список тегов
5. **Сохранение данных**: Извлеченные данные добавляются в глобальный словарь `quote_dct`.
6. **Сохранение в JSON**: После завершения сбора данных, они сохраняются в JSON-файл с помощью библиотеки `json`.

## Почему был выбран тот или иной метод/инструмент, а не другой

### 1. **Библиотека `requests`**

- **Простота использования**: Библиотека `requests` предоставляет удобный интерфейс и подходит для парсинга данного сайта.

### 2. **Библиотека `aiohttp`**

- **Асинхронность**: `aiohttp` позволяет выполнять асинхронные HTTP-запросы, что очень сильно ускоряет процесс скрапинга, в особенности при работе с большим количеством страниц.

### 3. **Библиотека `BeautifulSoup`**

- **Простота парсинга HTML**: `BeautifulSoup` предоставляет понятные и удобные методы для парсинга HTML и извлечения данных из DOM-дерева.

